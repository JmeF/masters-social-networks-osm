{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master's network summative\n",
    "\n",
    "This code was used in my MSc to produce a paper about OSM Forum networks. It was based on data scraped from the OSM Forum in a previous project and then used as the basis for a more complete analysis as part of my MSc thesis.\n",
    "\n",
    "For more information see [the full paper](https://docs.google.com/document/d/12grM5MfXpo88YOP7FeABqvumwlrL_wyY9rUpt5oZLD8/edit?usp=sharing) and the [short write up](https://www.jamiefawcett.org.uk/project/social-networks-osm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import ast\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import\n",
    "\n",
    "Data was collected by scraping the online [OSM Forum](https://forum.openstreetmap.org/) and contains a record of each post made to the forum, the time it was made, the topic, the author and various other details. More details can be found on my [projects page](https://www.jamiefawcett.org.uk/projects/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_csv(\"Clean_2019-03-10.csv\",index_col=0)\n",
    "\n",
    "#remove the rogue index and the additional details (as they now have their own columns)\n",
    "for col in ['index', 'additional']:\n",
    "    del full_data[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(full_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting unique authors (and linking them to OSM UIDs)\n",
    "\n",
    "Because display names on the Forum can change, we need to extract all the display names and link them to individual OSM accounts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique display name list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of unique authors with number of posts they have done (in data) + number of topics and sub-forums they have contributed to\n",
    "author = full_data.groupby(['author'])[['time','topic_title','forum_title']].nunique()\n",
    "making = [author]\n",
    "\n",
    "#for each additional detail we want\n",
    "additional = ['role','from','registr','num_post']\n",
    "for addit in additional:\n",
    "    addit_col = pd.Series(full_data.groupby(['author'])[addit].unique())\n",
    "    making.append(addit_col)\n",
    "\n",
    "#make into a df and reset the index \n",
    "author_df = pd.concat(making, axis =1, sort= False)\n",
    "author_df = author_df.reset_index()\n",
    "\n",
    "#unpack the columns\n",
    "for addit in additional:\n",
    "    author_df[addit] = author_df[addit].map(lambda l: l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fix up dates and numbers\n",
    "#fix today and yesterday\n",
    "def makeDates(x):\n",
    "    if \"Today\" in x:\n",
    "        x = x.replace(\"Today\",\"2019-03-10\")\n",
    "    if \"Yesterday\" in x:\n",
    "        x = x.replace(\"Yesterday\",\"2019-03-09\")\n",
    "    x = datetime.strptime(x, '%Y-%m-%d')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 'registr' a date again\n",
    "print(type(author_df['registr'][0]))\n",
    "author_df['registr'] = author_df['registr'].map(lambda x: makeDates(x))\n",
    "print(type(author_df['registr'][0]))\n",
    "\n",
    "#make num post and int\n",
    "print(type(author_df['num_post'][0]))\n",
    "author_df['num_post'] = author_df['num_post'].map(lambda x: int(x))\n",
    "print(type(author_df['num_post'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(author_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get OSM UID using whosthat API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace punctuation with hex so that we can search using the API.\n",
    "\n",
    "# define a dictionary of escaped punctuation and its replacement\n",
    "hex_ver_plain = {}\n",
    "for punc in string.punctuation:\n",
    "    hex_ver_plain[re.escape(punc)] = punc.encode('utf-8').hex()\n",
    "\n",
    "#use this to come up with a pattern\n",
    "pattern = re.compile(r'|'.join(key for key in hex_ver_plain.keys()))\n",
    "\n",
    "#create another dictionary with compile versions of that\n",
    "hexver = {}\n",
    "for punc in string.punctuation:\n",
    "    hexver[re.compile(re.escape(punc))] = \"%{}\".format(punc.encode('utf-8').hex())\n",
    "\n",
    "#define a function that allows us to use this dictionary to replace \n",
    "def encode_function(matchobj): \n",
    "    repl = hexver[matchobj.re]\n",
    "    return matchobj.expand(repl)\n",
    "\n",
    "#use this function to actually do it\n",
    "def convert_to_hex(name):\n",
    "    to_replace = re.findall(pattern,name)\n",
    "    for punc in to_replace:\n",
    "        name = re.sub(re.escape(punc),encode_function, name)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_df['search_name'] = author_df['author'].map(lambda x: convert_to_hex(x))\n",
    "# replace spaces with '+'\n",
    "author_df['search_name'] = author_df['search_name'].map(lambda x: re.sub(\" \",\"+\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(author_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get UIDs from the whosthat API\n",
    "id_list = []\n",
    "\n",
    "for name in author_df['search_name']:\n",
    "    details = {}\n",
    "    api_location = \"http://whosthat.osmz.ru/whosthat.php?action=info&name={}\".format(name)\n",
    "    response = requests.get(api_location)\n",
    "    response_text = response.text\n",
    "    result = json.loads(response_text)\n",
    "    if result == []:\n",
    "        details['id'] = None\n",
    "        details['all_names'] = None\n",
    "    else:\n",
    "        details['id'] = result[0]['id']\n",
    "        details['all_names'] = result[0]['names']\n",
    "    details_df = pd.DataFrame([details.values()],index=[name], columns=details.keys())\n",
    "    id_list.append(details_df)\n",
    "\n",
    "id_df = pd.concat(id_list,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(id_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine with unique author dataframe\n",
    "\n",
    "Now we need to put current display name (and ID) into the main DF and rerun the aggregation function by current_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the two dataframes to get a full dataframe with ID and author in\n",
    "id_author_df = author_df.merge(id_df,left_on=\"search_name\",right_index=True)\n",
    "print(len(id_author_df[id_author_df['id'].isnull()])) #missing 5582 (may not have made any map edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the display names used (if multiple use dictionary)\n",
    "#function to extract names with dates so all the same\n",
    "def getAllNames(listed_name,all_names):\n",
    "    if all_names is None or len(all_names) == 1:\n",
    "        full_names = {}\n",
    "        full_names[listed_name] = 1\n",
    "    else:\n",
    "        full_names = {}\n",
    "        for item in all_names: #in the list\n",
    "            alt_name = item['name']\n",
    "            alt_date = item['last']\n",
    "            full_names[alt_name] = alt_date\n",
    "    return full_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_author_df['recent_names'] = id_author_df.apply(lambda x: getAllNames(x.loc['author'],x.loc['all_names']),axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the current name:\n",
    "def getCurrentName(recent_names):\n",
    "    current = max(recent_names.keys(), key=(lambda key: recent_names[key]))\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_author_df['current_name'] = id_author_df['recent_names'].map(lambda x: getCurrentName(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPastNames(current_name,all_names):\n",
    "    if all_names is None or len(all_names) == 1:\n",
    "        return []\n",
    "    else:\n",
    "        extra_names = []\n",
    "        for item in all_names: #in the list\n",
    "            if item['name'] != current_name: #name in the dictionary is not current display name\n",
    "                extra_names.append(item['name'])\n",
    "        return extra_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_author_df['past_names'] = id_author_df.apply(lambda x: getPastNames(x.loc['current_name'],x.loc['all_names']),axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(id_author_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(id_author_df))\n",
    "print(id_author_df['current_name'].nunique())\n",
    "print(id_author_df['id'].nunique() + len(id_author_df[id_author_df['id'].isnull()]))\n",
    "print(id_author_df['id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a sparse version of this to insert into the full DF\n",
    "check_author_df = id_author_df[['author','current_name','id','past_names']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge this into the main dataframe\n",
    "full_id_df = full_data.merge(check_author_df,on=\"author\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save this\n",
    "#full_id_df.to_csv('Full_wID.csv', mode ='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remake the author DF (to get edits/changeset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(full_id_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of unique current_names with number of posts they have done (in data) + number of topics and sub-forums they have contributed to\n",
    "current_name = full_id_df.groupby(['current_name'])[['time','topic_title','forum_title']].nunique()\n",
    "making = [current_name]\n",
    "\n",
    "#for each additional detail we want (except past_names but we do author instead)\n",
    "additional = ['role','from','registr','num_post','id','author'] #,'past_names'\n",
    "for addit in additional:\n",
    "    addit_col = pd.Series(full_id_df.groupby(['current_name'])[addit].unique())\n",
    "    making.append(addit_col)\n",
    "    \n",
    "#make into a df and reset the index \n",
    "unique_author_df = pd.concat(making, axis =1, sort= False)\n",
    "unique_author_df = unique_author_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpack the columns which have only one value\n",
    "for addit in ['id','role']:\n",
    "    unique_author_df[addit] = unique_author_df[addit].map(lambda l: l[0])\n",
    "\n",
    "#choose the earliest registr\n",
    "unique_author_df['early_registr'] = unique_author_df['registr'].map(lambda x: min(x))\n",
    "\n",
    "#sum contributions\n",
    "unique_author_df['total_posts'] = unique_author_df['num_post'].map(lambda x: int(sum(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(unique_author_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get edits + reg date from OSM public profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_find(element, error_message, *args, **kargs):\n",
    "    try:\n",
    "        return element.find(*args, **kargs)\n",
    "    except:\n",
    "        return error_message\n",
    "\n",
    "def safe_findAll(element, error_message, *args, **kargs):\n",
    "    try:\n",
    "        return element.findAll(*args, **kargs)\n",
    "    except:\n",
    "        return error_message\n",
    "\n",
    "def getEdit(current_name):\n",
    "    edit_dict = {}\n",
    "    profile_url = \"https://www.openstreetmap.org/user/{}\".format(current_name)\n",
    "    response = requests.get(profile_url).text\n",
    "    soup = bs(response)\n",
    "    content = soup.find(\"div\",{\"class\":\"content-inner\"})\n",
    "    user_info = safe_find(content,None,\"div\",{\"class\":\"userinformation-inner\"})\n",
    "    if user_info != None:\n",
    "        joined = safe_find(user_info,None,\"p\",{\"class\":\"deemphasize\"}).small.text\n",
    "        joined = re.sub(\"Mapper since: \",\"\",joined).strip()\n",
    "        edit_count = safe_find(user_info,None,\"span\",{\"class\":\"count-number\"}).text\n",
    "        edit_count = int(re.sub(\",\",\"\",edit_count))\n",
    "        edit_dict['joined'] = joined\n",
    "        edit_dict['num_edits'] = edit_count\n",
    "    else:\n",
    "        edit_dict['joined'] = None\n",
    "        edit_dict['num_edits'] = None\n",
    "    return edit_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a new dataframe\n",
    "current_list = unique_author_df['current_name']\n",
    "total_number = len(current_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits_list = []\n",
    "for c,name in enumerate(current_list):\n",
    "    edits = getEdit(name)\n",
    "    edit_series = pd.DataFrame([edits.values()],index=[name], columns=edits.keys())\n",
    "    edits_list.append(edit_series)\n",
    "    through = ((c+1) / total_number) * 100\n",
    "    if c % 100 == 0:\n",
    "        print(\"{}% through\".format(through))\n",
    "edits_df = pd.concat(edits_list,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(edits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge this back in\n",
    "full_num_df = unique_author_df.merge(edits_df,left_on=\"current_name\",right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove \" | Contributor terms: Undecided\" if present and make a date\n",
    "def removeContrib(date):\n",
    "    if date != None:\n",
    "        clean = date.split(\"\\n            |\", 1)[0]\n",
    "        clean_date = datetime.strptime(clean, '%B %d, %Y')\n",
    "        return clean_date\n",
    "    else: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_num_df['joined_osm'] = full_num_df['joined'].map(lambda x: removeContrib(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(full_num_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(full_num_df[full_num_df['num_edits'].isnull()])) #267 have no record on OSM (might be private profiles)\n",
    "print(len(full_num_df[full_num_df['id'].isnull()])) # 5582 have no id associated (havent made an edit since switching name?)\n",
    "print(len(full_num_df[full_num_df['time']!=full_num_df['total_posts']])) #531 have a different number of posts than counted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_names = full_num_df.columns.values\n",
    "desired_dict = {'current_name':'current_name',\n",
    "                 'time':'posts_found',\n",
    "                 'topic_title':'num_topics',\n",
    "                 'forum_title':'num_forums',\n",
    "                 'role':'role',\n",
    "                 'from':'from_list',\n",
    "                 'registr':'registr_list',\n",
    "                 'num_post':'post_list',\n",
    "                 'id':'osm_id',\n",
    "                 'author':'past_names',\n",
    "                 'early_registr':'forum_registr',\n",
    "                 'total_posts':'total_posts',\n",
    "                 'joined':'joined_dirty',\n",
    "                 'num_edits':'num_edits',\n",
    "                 'joined_osm':'joined_osm'}\n",
    "set_names = [desired_dict[x] for x in current_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_num_df.columns = set_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_num_df.to_csv('authors_with_num_edits.csv', mode = 'w') ##maybe make categorical here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all author details with the forum posts dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again create a simplified version that can be merged back into the main dataframe\n",
    "check_author_edit_df = full_num_df[['current_name','past_names','role','from_list','forum_registr','total_posts','osm_id','joined_osm','num_edits','edit_cat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(check_author_edit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If loading data with list in a csv - need to remake list\n",
    "def remakeList(x):\n",
    "    y = re.sub(\"' '\",\"','\",x)\n",
    "    y = ast.literal_eval(y)\n",
    "    return y\n",
    "check_author_edit_df['past_names_list'] = check_author_edit_df['past_names'].map(lambda x: remakeList(x))\n",
    "type(check_author_edit_df['past_names_list'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary to join all current names (in the check_author_edit_df) to display names (in main df)\n",
    "df2 = pd.DataFrame(check_author_edit_df['past_names_list'].tolist(), index=[check_author_edit_df['current_name']]).stack().reset_index(level=1, drop=True).reset_index()\n",
    "df2.columns = [\"current_name\",\"author\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df2))\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then merge that with the full list of unique authors so there is a record for each display name that links to the current name \n",
    "check_author_edit_full_df = df2.merge(check_author_edit_df,on =\"current_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge this into the main dataframe\n",
    "full_id_num_df = full_data.merge(check_author_edit_full_df,on=\"author\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(full_id_num_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_id_num_df.to_csv('Full_with_ID+num_edits.csv', mode = 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv('Full_with_ID+num_edits.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(full_df.head())\n",
    "display(full_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fix up dates and numbers\n",
    "#fix today and yesterday\n",
    "def makeDates(x):\n",
    "    if \"Today\" in x:\n",
    "        x = x.replace(\"Today\",\"2019-03-10\")\n",
    "    if \"Yesterday\" in x:\n",
    "        x = x.replace(\"Yesterday\",\"2019-03-09\")\n",
    "    x = datetime.datetime.strptime(x, '%Y-%m-%d')\n",
    "    return x\n",
    "\n",
    "# Make 'registr' a date again\n",
    "print(type(full_df['registr'][0]))\n",
    "full_df['registr'] = full_df['registr'].map(lambda x: makeDates(x))\n",
    "print(type(full_df['registr'][0]))\n",
    "\n",
    "#convert time to a timestamp again\n",
    "full_df['time'] = full_df['time'].map(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "def makeStr(x):\n",
    "    if pd.isnull(x):\n",
    "        return x\n",
    "    else: return str(int(round(x)))\n",
    "\n",
    "#make OSM_id a str not a float\n",
    "full_df['osm_id_str'] = full_df['osm_id'].map(lambda x: makeStr(x) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make edits caregorical\n",
    "\n",
    "Because edits are power-law distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of edits as log\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#shows distribution\n",
    "contrib_plot = sns.distplot(full_num_df['num_edits'].dropna(),color=\"blue\",kde=False)\n",
    "plt.yscale('log')\n",
    "# plt.xscale('log')\n",
    "contrib_plot.set_title(\"Histogram of contributions by number of contributors\") #(log) \n",
    "contrib_plot.set(xlabel=\"Number of contributions\",ylabel=\"Number of contributors\")\n",
    "# fig = contrib_plot.get_figure()\n",
    "# fig.savefig(\"contributor_hist.png\")\n",
    "plt.show()\n",
    "\n",
    "#read out of groups - same size-ish\n",
    "print(len(full_num_df['num_edits']))\n",
    "print()\n",
    "print(\"NA:\",len(full_num_df[full_num_df['num_edits'].isnull()]))\n",
    "print(\"None:\",len(full_num_df[full_num_df['num_edits']==0]))\n",
    "print(\"Less than 10:\",len(full_num_df[full_num_df['num_edits']<=10]))\n",
    "print(\"Between 10 and 100:\",len(full_num_df[(full_num_df['num_edits']>10)&(full_num_df['num_edits']<=100)]))\n",
    "print(\"Between 100 and 1000:\",len(full_num_df[(full_num_df['num_edits']>100)&(full_num_df['num_edits']<1000)]))\n",
    "print(\"Between 1000 and 10000:\",len(full_num_df[(full_num_df['num_edits']>1000)&(full_num_df['num_edits']<10000)]))\n",
    "print(\"Over 10000:\",len(full_num_df[full_num_df['num_edits']>10000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###make edits categorical\n",
    "def bin_edits(x):\n",
    "    if pd.isnull(x):\n",
    "        result = None\n",
    "    elif x == 0:\n",
    "        result = \"No edits\"\n",
    "    elif (x > 0 ) & (x <= 10):\n",
    "        result = \"Less than 10\"\n",
    "    elif (x>10) & (x<=100):\n",
    "        result = \"Between 10 and 100\"\n",
    "    elif (x>100) & (x<=1000):\n",
    "        result = \"Between 100 and 1000\"\n",
    "    elif (x>1000) & (x<=10000):\n",
    "        result = \"Between 1000 and 10000\"\n",
    "    elif x>10000:\n",
    "        result = \"Over 10000\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['edit_cat'] = full_df['num_edits'].map(lambda x: str(bin_edits(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an attribute dict here for the nodes (would be better lower down but oh well)\n",
    "full_df['attr'] = full_df.apply(lambda x: {'role':x['role_y'],'osm_id':str(x['osm_id_str']),\n",
    "                                           'num_edits':x['num_edits'], 'num_edits_cat':x['edit_cat_str'],\n",
    "                                           'num_posts':x['num_post'],'forum_registr':x['forum_registr'], 'joined_osm':x['joined_osm']},axis=1) \n",
    "#coerced to string to avoid nonetype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for making dates, networks and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allTime(data = full_df,jump=1):\n",
    "    total_start = pd.Timestamp(\"2007-12-16 00:00:00\")\n",
    "    total_end = (max(data['time']) + datetime.timedelta(days=1) + pd.DateOffset(hour=1,normalize=True)) # want midnight on the day after data collection - datetime.timedelta(seconds=1) #to one second to mid that day\n",
    "    date_range = pd.date_range(start= total_start,end = total_end - datetime.timedelta(days=30),freq=pd.DateOffset(days=jump),closed=\"right\")\n",
    "    return date_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that makes all the good stuff given a start date\n",
    "def makeData(start_date,period):\n",
    "    data = {'start':start_date}\n",
    "    end_date = start_date + datetime.timedelta(days=period)\n",
    "    mask = full_df['time'].between(start_date,end_date)\n",
    "    \n",
    "    window_df = full_df[mask]\n",
    "\n",
    "    ## unique authors in dataframe (bottom nodelist)\n",
    "    data['authors_list'] = list(window_df['current_name'].unique())\n",
    "\n",
    "    ## unique authors attributes\n",
    "    data['authors_attrib'] = dict(zip(window_df['current_name'], window_df['attr']))\n",
    "\n",
    "    ## unique topics in dataframe (top nodelist)\n",
    "    data['topics_list'] = list(window_df['topic_title'].unique())\n",
    "\n",
    "    ## edge list between the two\n",
    "    bi_edge_df = window_df[['topic_title','current_name']].groupby(['topic_title','current_name'])['current_name'].count()\n",
    "    bi_edge_df = bi_edge_df.rename(columns={'current_name':'count'}).reset_index().rename(columns={0:'count'})\n",
    "    bi_edge_df['edge'] = bi_edge_df.apply(lambda x: (x['topic_title'],x['current_name']),axis=1)\n",
    "    data['edge_list'] = list(bi_edge_df['edge'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "#function that builds only the projected graph from the data\n",
    "def buildOnlyProjGraph(data):\n",
    "    #build bipartite\n",
    "    B = nx.Graph(name = data['start'])\n",
    "    B.add_nodes_from(data['topics_list'], bipartite=0) # top nodes\n",
    "    B.add_nodes_from(data['authors_list'], bipartite=1) # bottom nodes\n",
    "    B.add_edges_from(data['edge_list'])\n",
    "    top_nodes = {n for n, d in B.nodes(data=True) if d['bipartite']==0}\n",
    "    bottom_nodes = set(B) - top_nodes\n",
    "    \n",
    "    #build full weighted projected\n",
    "    G = bipartite.weighted_projected_graph(B, bottom_nodes)\n",
    "    nx.set_node_attributes(G, data['authors_attrib']) #may actually want to remove attribute where 'none'?\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics functions \n",
    "def gini(list_of_values):\n",
    "    sorted_list = sorted(list_of_values)\n",
    "    height, area = 0, 0\n",
    "    for value in sorted_list:\n",
    "        height += value\n",
    "        area += height - value / 2.\n",
    "    fair_area = height * len(list_of_values) / 2.\n",
    "    return (fair_area - area) / fair_area\n",
    "\n",
    "\n",
    "def partition_from_attr(G, attr):\n",
    "    # Part 1. We create a numeric mapping between attribute and\n",
    "    # the partition. We will print this out afterwards. \n",
    "    \n",
    "    attr_set = set([])\n",
    "    for i in G.nodes: \n",
    "        if attr in G.nodes[i]:\n",
    "            attr_set.add(G.nodes[i][attr])    \n",
    "    attr_dict = {j:i for i,j in enumerate(attr_set)}\n",
    "        \n",
    "    part_dict = {}\n",
    "    for i in G.nodes: \n",
    "        if attr in G.nodes[i]:\n",
    "            part_dict[i] = attr_dict[ G.nodes[i][attr] ]\n",
    "        else:\n",
    "            part_dict[i] = len(part_dict)\n",
    "    \n",
    "    return part_dict\n",
    "\n",
    "# function that calculates metrics for the whole graph\n",
    "def getMetrics(g,name):\n",
    "    giant = max(nx.connected_component_subgraphs(g),key=len)\n",
    "    metrics = {}\n",
    "    \n",
    "    #1 -- Participatory\n",
    "    #basics\n",
    "    metrics['1_num_nodes'] = len(g.nodes)\n",
    "    metrics['1_num_edges'] = g.size()\n",
    "    metrics['1_weight_edges'] = g.size(weight='weight')\n",
    "    ## Percentage isolates, giant component, other\n",
    "    metrics['1_per_iso'] = len([node for node in g.nodes() if g.degree[node] == 0]) /len(g.nodes)\n",
    "    #Percentage in giant component\n",
    "    metrics['1_per_giant'] = len(giant)/len(g.nodes)\n",
    "    \n",
    "    \n",
    "    #2 -- influence inequality\n",
    "    \n",
    "    ##inequality measure of interactions\n",
    "    giant_deg_seq = sorted([d for n, d in giant.degree(weight=\"weight\")], reverse=True)\n",
    "    metrics['2_giant_gini'] = gini(giant_deg_seq)\n",
    "    \n",
    "    ##unique connections\n",
    "    giant_deg_seq_unweight = sorted([d for n, d in giant.degree()], reverse=True)\n",
    "    metrics['2_giant_gini_unweight'] = gini(giant_deg_seq_unweight)\n",
    "    \n",
    "    ## % contributions by top 10\n",
    "    wdegree = pd.Series(dict(g.degree(weight=\"weight\"))).sort_values(ascending=False)\n",
    "    top10 = wdegree[0:(round(len(wdegree)/10) - 1)]\n",
    "    metrics['2_top10_per_contrib'] = top10.sum()/wdegree.sum()\n",
    "    \n",
    "    ## % contributions by top 10 unweighted\n",
    "    degree = pd.Series(dict(g.degree())).sort_values(ascending=False)\n",
    "    top10_unweigh = degree[0:(round(len(degree)/10) - 1)]\n",
    "    metrics['2_top10_per_contrib_unweigh'] = top10_unweigh.sum()/degree.sum()\n",
    "    \n",
    "    # 3 -- meritocracy\n",
    "    \n",
    "    # edit assortativity\n",
    "    attribute = \"num_edits_cat\"\n",
    "\n",
    "    try:\n",
    "        nodelist = [node_name for node_name in giant.nodes if giant.nodes[node_name][attribute] != \"None\"]\n",
    "        metrics['3_{}_assort'.format(attribute)] = nx.attribute_assortativity_coefficient(giant,attribute,nodelist)\n",
    "    except KeyError:\n",
    "        metrics['3_{}_assort'.format(attribute)] = None\n",
    "        \n",
    "    try:\n",
    "        metrics['3_{}_assort_unfilt'.format(attribute)] = nx.attribute_assortativity_coefficient(giant,attribute)\n",
    "    except KeyError:\n",
    "        metrics['3_{}_assort_unfilt'.format(attribute)] = None\n",
    "    \n",
    "    # Atribute modularity\n",
    "    attrib_part = partition_from_attr(giant,\"num_edits_cat\")\n",
    "    metrics['3_edit_modularity'] = community.modularity(attrib_part,giant)\n",
    "    \n",
    "    #weighted degree assortativity\n",
    "    metrics['3_giant_weightDA'] = nx.degree_assortativity_coefficient(giant,weight=\"weight\")\n",
    "    \n",
    "    #4 -- Decentralisation\n",
    "    \n",
    "    ##identfying communities\n",
    "    part = community.best_partition(giant,weight='weight')\n",
    "    metrics['4_comm_modularity'] = community.modularity(part,giant)\n",
    "    \n",
    "    metrics['4_num_communities'] = len(pd.Series(part).unique())\n",
    "    metrics['4_comm_size_gini'] = gini(list(pd.Series(part).value_counts()))\n",
    "    \n",
    "    #5 -- persistence\n",
    "    \n",
    "    metrics['5_all_nodes'] = list(g.nodes())\n",
    "    metrics['5_giant_nodes'] = list(giant.nodes()) #list added here but not checked\n",
    "    metrics['5_top10_nodes'] = list(top10.index)\n",
    "       \n",
    "    \n",
    "    #RANDOM SHIT\n",
    "    ##clustering coefficient\n",
    "    metrics[\"cluster_coeff\"] = nx.average_clustering(giant)\n",
    "    ## Transitivity - tells us the number of open two paths in all potential triangles\n",
    "    metrics[\"transitivity\"] = nx.transitivity(giant)\n",
    "    ## Density\n",
    "    metrics[\"density\"] = nx.density(giant)\n",
    "    \n",
    "    metrics_df = pd.DataFrame([metrics.values()],index=[name], columns=metrics.keys())\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary function that rolls them all together\n",
    "def getAll(date_list):\n",
    "    metric_list = []\n",
    "    for c,date in enumerate(date_list):\n",
    "        data = makeData(date, 30)\n",
    "#         graphs = buildGraphs(data)\n",
    "        graph = buildOnlyProjGraph(data)\n",
    "        metrics = getMetrics(graph,data['start'])\n",
    "        metric_list.append(metrics)\n",
    "        if c % 30 == 0:\n",
    "            print(\"{:.2%}\".format((c+1)/len(date_list)))\n",
    "    full_metric_df = pd.concat(metric_list, axis =0)\n",
    "    return full_metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#every 15 days \n",
    "all_days_15 = allTime(full_df,15)\n",
    "all_days_15_df = getAll(all_days_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(all_days_15_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### making Jaccard similarity (with a shift of 2 to create non-overlapping windows)\n",
    "\n",
    "#shift the index by one to give the previous score\n",
    "all_days_15_df['5_all_nodes_previous'] = all_days_15_df['5_all_nodes'].shift(2)\n",
    "all_days_15_df['5_giant_nodes_previous'] = all_days_15_df['5_giant_nodes'].shift(2)\n",
    "all_days_15_df['5_top10_nodes_previous'] = all_days_15_df['5_top10_nodes'].shift(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Jaccard similarity \n",
    "\n",
    "def makeSim(past,present):\n",
    "    if isinstance(past,list):\n",
    "        present_s = set(present)\n",
    "        past_s = set(past)\n",
    "        jac_sim = len(present_s.intersection(past_s)) / len(present_s.union(past_s))\n",
    "    else:\n",
    "        jac_sim = None\n",
    "    return jac_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_days_15_df['5_all_nodes_jac_sim'] = all_days_15_df.apply(lambda x: makeSim(x.loc['5_all_nodes_previous'],x.loc['5_all_nodes']),axis=1) \n",
    "all_days_15_df['5_giant_nodes_jac_sim'] = all_days_15_df.apply(lambda x: makeSim(x.loc['5_giant_nodes_previous'],x.loc['5_giant_nodes']),axis=1) \n",
    "all_days_15_df['5_top10_nodes_jac_sim'] = all_days_15_df.apply(lambda x: makeSim(x.loc['5_top10_nodes_previous'],x.loc['5_top10_nodes']),axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(all_days_15_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_days_15_df.to_csv('final_network_output.csv', mode = 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unused graph building functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that builds a Bipartite graph from the data\n",
    "def buildAllGraphs(data,weight=2):\n",
    "    #build bipartite\n",
    "    B = nx.Graph(name = data['start'])\n",
    "    B.add_nodes_from(data['topics_list'], bipartite=0) # top nodes\n",
    "    B.add_nodes_from(data['authors_list'], bipartite=1) # bottom nodes\n",
    "    B.add_edges_from(data['edge_list'])\n",
    "    top_nodes = {n for n, d in B.nodes(data=True) if d['bipartite']==0}\n",
    "    bottom_nodes = set(B) - top_nodes\n",
    "    \n",
    "    #build full weighted projected\n",
    "    G = bipartite.weighted_projected_graph(B, bottom_nodes)\n",
    "    nx.set_node_attributes(G, data['authors_attrib']) # may actually want to remove attribute where 'none'?\n",
    "    \n",
    "    #build cut-off weighted projected\n",
    "    GG = G.copy()\n",
    "    to_drop_list = [i for i in GG.edges if GG.edges[i][\"weight\"] < weight]\n",
    "    GG.remove_edges_from( to_drop_list)\n",
    "\n",
    "    to_drop_list = [i for i in GG.nodes if GG.degree[i]  < 1]\n",
    "    GG.remove_nodes_from( to_drop_list)\n",
    "    \n",
    "    graphs = {'bipartite':B,'simple_projected':G,'cut_projected':GG,'top_nodes':top_nodes,'bottom_nodes':bottom_nodes}\n",
    "\n",
    "#     print(nx.info(B))\n",
    "#     print()\n",
    "#     print(nx.info(G))\n",
    "#     print()\n",
    "#     print(nx.info(GG))\n",
    "#     print()\n",
    "\n",
    "    return graphs\n",
    "\n",
    "#function that visualises graphs as desired -- make it better\n",
    "def visGraphs(graphs,bipart=True,sim_proj=True,cut_proj=True):\n",
    "    B = graphs['bipartite']\n",
    "    G = graphs['simple_projected']\n",
    "    GG = graphs['cut_projected']\n",
    "    if bipart == True:\n",
    "        pos=nx.spring_layout(B)\n",
    "        \n",
    "        #separates the nodes\n",
    "        for node_id in graphs['top_nodes']:\n",
    "            layout = pos[node_id]\n",
    "            pos[node_id] = np.array([layout[0],(layout[1]+2)])\n",
    "\n",
    "        nx.draw_networkx_nodes(B,pos,graphs['top_nodes'],node_color=\"skyblue\",node_shape=\"D\",alpha=.8,edgecolors=\"black\")\n",
    "        nx.draw_networkx_nodes(B,pos,graphs['bottom_nodes'],node_color=\"lightgreen\",node_shape=\"s\",alpha=.8,edgecolors=\"black\")\n",
    "        # nx.draw_networkx_labels(gbp, pos)\n",
    "        nx.draw_networkx_edges(B,pos,alpha=.2)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    if sim_proj == True:\n",
    "        pos_forum =nx.spring_layout(G) \n",
    "\n",
    "        nx.draw_networkx_nodes(G,pos_forum,node_size=2,node_color=\"skyblue\",alpha=.8)\n",
    "        nx.draw_networkx_edges(G,pos_forum,width=1,alpha=.2)\n",
    "        # nx.draw_networkx_labels(B,pos_forum,font_size=20,font_color='b',font_family='sans-serif')\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    if cut_proj == True:\n",
    "        pos_forum =nx.spring_layout(GG) \n",
    "\n",
    "        nx.draw_networkx_nodes(GG,pos_forum,node_size=2,node_color=\"lightgreen\",alpha=.8)\n",
    "        nx.draw_networkx_edges(GG,pos_forum,width=1,alpha=.2)\n",
    "        # nx.draw_networkx_labels(B,pos_forum,font_size=20,font_color='b',font_family='sans-serif')\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
